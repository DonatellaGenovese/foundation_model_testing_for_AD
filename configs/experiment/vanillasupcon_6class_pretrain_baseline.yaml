# @package _global_

# Baseline: evaluate probes on an untrained (randomly initialized) SupCon encoder.
# This lets you compare the quality of learned embeddings vs. no contrastive training.
#
# Run with e.g.:
#   python src/train.py experiment=vanillasupcon_6class_pretrain_baseline paths=fm_testing_4090 data=collide2v_miniA6000

defaults:
  - override /data: collide2v_mini4090
  - override /callbacks: supcon
  - override /trainer: gpu
  - override /logger: mlflow
  - override /model: vanilla_supcon

# Tags and seed
tags: ["collide2v", "vanillasupcon", "baseline", "6class", "metric_learning"]

seed: 42

# Use exactly the same data setup as vanillasupcon_6class_pretrain
data:
  batch_size: 512
  train_val_test_split_per_class: [80000, 10000, 10000]
  label: "v2_6class_highlevel"
  num_workers: 7
  to_classify:
    - "QCD_inclusive"
    - "DY_to_ll"
    - "Z_to_bb"
    - "W_to_lv"
    - "gamma"
    - "tt_all-lept"
  datasets_config:
    jets:
      cols:
        - FullReco_JetPuppiAK4_PT
        - FullReco_JetPuppiAK4_Eta
        - FullReco_JetPuppiAK4_Phi
        - FullReco_JetPuppiAK4_Mass
        - FullReco_JetPuppiAK4_BTag
        - FullReco_JetPuppiAK4_Charge
      topk: 12
      count: true

    electrons:
      cols:
        - FullReco_Electron_PT
        - FullReco_Electron_Eta
        - FullReco_Electron_Phi
        - FullReco_Electron_EhadOverEem
        - FullReco_Electron_IsolationVarRhoCorr
      topk: 8
      count: false

    muons:
      cols:
        - FullReco_MuonTight_PT
        - FullReco_MuonTight_Eta
        - FullReco_MuonTight_Phi
        - FullReco_MuonTight_IsolationVarRhoCorr
      topk: 8
      count: false

    photons:
      cols:
        - FullReco_PhotonTight_PT
        - FullReco_PhotonTight_Eta
        - FullReco_PhotonTight_Phi
      topk: 8
      count: false

    puppi_met:
      cols:
        - FullReco_PUPPIMET_MET
        - FullReco_PUPPIMET_Phi
      topk: null
      count: false

# Same model hyperparameters as training experiment,
# but we won't run contrastive training (train=False).
model:
  n_heads: 8
  projection_dim: 128
  hidden_projection_dim: 256
  temperature: 0.07
  base_temperature: 1.0
  use_classification_head: false

  optimizer:
    lr: 0.0003
    weight_decay: 0.00001

  scheduler:
    T_max: ${trainer.max_epochs}
    eta_min: 0.000001

# Trainer is still instantiated but training/testing will be skipped.
trainer:
  min_epochs: 2
  max_epochs: 10
  gradient_clip_algorithm: norm
  gradient_clip_val: 1.0

# Don't train or test; just run probe evaluation on the untrained encoder.
train: false
test: false

preprocess:
  enabled: true

logger:
  mlflow:
    experiment_name: "supcon_pretraining"
    run_name: "vanillasupcon_6class_pretrain_baseline"

# Run automatic probe evaluation using the randomly initialized encoder
eval_after_training: true

eval:
  linear_probe:
    max_epochs: 15
    lr: 0.001
    weight_decay: 0.0

  knn_probe:
    k_values: [1, 3, 5, 10, 20]
    metric: "cosine"

  output_dir: ${paths.output_dir}/probe_evaluation_baseline
